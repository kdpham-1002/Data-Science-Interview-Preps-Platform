{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Interview Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1: Series vs Dataframe\n",
    "\n",
    "> Q1: What is the difference between a **series** and a **dataframe** in Pandas?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Series is one-dimensional, while DataFrame is two-dimensional\n",
    "* Series can be seen as a single column of data, while DataFrame is a combination of multiple Series.\n",
    "\n",
    "Bonus: When working with DataFrames you'll often select a single Series to manipulate and join back to the original DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2: loc vs iloc\n",
    "\n",
    "> Q2: What is the difference between the **loc** and **iloc** methods in Pandas?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* \"iloc\" is used for ***integer-location*** based indexing/selection by position. This means that you use the integer\n",
    "index of the row or column you want to access (starting from 0).\n",
    "* \"loc\" is used for ***label-location*** based indexer for selection by\n",
    "label. This means that you use the named index of the row\n",
    "or column you want to access."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3: axis parameter\n",
    "\n",
    "> Q3: Explain the use of **'axis ='** parameter in Pandas?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* axis=0 will be applied along the \"row\" direction\n",
    "* axis=1 will be applied along the \"column\" direction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4: inplace parameter\n",
    "\n",
    "> Q4: Explain the use of **'inplace ='** parameter in Pandas?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If you perform an operation on a DataFrame and don't use\n",
    "inplace, a modified copy of that DataFrame is created.\n",
    "* If you set ***'inplace = True'***, the operation will modify the\n",
    "DataFrame itself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Q5: Missing data\n",
    "\n",
    "> Q5: How can you handle missing data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Drop Missing Values using \"dropna()\"\n",
    "2. Fill missing Values with specific values using \"fillna()\"\n",
    "3. Fill missing Values with computer values (mean, median, mode) using \"fillna().mean()\"\n",
    "4. Using the Interopolation with interopolatel() method\n",
    "\n",
    "Bonus: Interopolation is especially useful with time series data and estimates values between two known values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6: Large data sets\n",
    "\n",
    "> Q6: How can you handle very large data sets that don't fit into memory?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can handle very large data sets that don't fit into memory using several techniques:\n",
    "1.\tProcess Data in Chunks  \n",
    "•\tWhy: Instead of loading a huge file all at once, you process smaller parts (chunks) of it so you don’t overload your memory.  \n",
    "•\tPandas (use read_csv(..., chunksize=...))  \n",
    "\n",
    "2.\tSample for Exploration  \n",
    "•\tWhy: You can examine a smaller, representative subset of data for quick analysis before working with the full dataset.  \n",
    "•\tPandas (e.g., df.sample(n=1000))  \n",
    "•\tSQL (TABLESAMPLE clause in some databases)  \n",
    "\n",
    "3.\tStore Data in a Database  \n",
    "•\tWhy: Databases efficiently handle large datasets and let you query only the parts you need.  \n",
    "•\tMySQL, PostgreSQL, SQLite (Databases)  \n",
    "•\tSQLAlchemy or pandas read_sql_query (to connect Python/Pandas with databases)  \n",
    "\n",
    "4.\tUse Distributed Computing  \n",
    "•\tWhy: If your dataset is extremely large, chunking might still be too slow. A cluster of machines can split up the work.  \n",
    "•\tApache Spark (via PySpark) automatically distributes your data across a cluster and processes it in parallel.  \n",
    "•\tDask offers a more “Pandas-like” interface but still runs across multiple machines or cores.  \n",
    "•   Databricks: A managed platform that makes it easier to run Spark in the cloud without handling cluster setup yourself.\n",
    "\n",
    "5.\tUse Efficient File Formats  \n",
    "•\tWhy: Formats like Parquet or ORC store data in a compressed, columnar layout, so you read less from disk and load only what you need.  \n",
    "•\tParquet, ORC  \n",
    "•\tpyarrow or fastparquet (for reading/writing parquet in Python)  \n",
    "\n",
    "6.\tStream Data in Real Time  \n",
    "•\tWhy: If data arrives continuously (like from sensors or a website), you process records on the fly instead of storing it all first.  \n",
    "•\tKafka, RabbitMQ (for real-time data pipelines)  \n",
    "•\tSpark Streaming, Apache Flink, Dask (for streaming computation)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q7: Handling New Data\n",
    "\n",
    "> Q7: What steps would you take to analyze a new, unfamiliar dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the steps I usually take:\n",
    "1. Importing Data into a Dataframe\n",
    "2. Preview data using head(), tail(), info(), describe()\n",
    "3. Check for missing values\n",
    "4. Data Cleaning & Feature Engineering\n",
    "5. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q8: Favorite Pandas Trick\n",
    "\n",
    "> Q8: What is your favorite \"trick\" function in Pandas that you find particularly useful?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### .query() method\n",
    "The query() method allows you to filter a DataFrame using a string expression. It’s often more readable than using standard boolean indexing.\n",
    "\n",
    "Pro tip: You can reference columns without quoting them as df['column']; just type the column name in your query string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Name  Age City\n",
      "0  Alice   24   NY\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'Name': ['Alice', 'Bob', 'Charlie', 'Diana'],\n",
    "    'Age': [24, 42, 18, 35],\n",
    "    'City': ['NY', 'LA', 'NY', 'Chicago']\n",
    "})\n",
    "\n",
    "## Using query() to filter rows where Age > 20 and City == 'NY'\n",
    "filtered_df = df.query(\"Age > 20 and City == 'NY'\")\n",
    "# filtered_df = df[(df['Age'] > 20) & (df['City'] == 'NY')]\n",
    "print(filtered_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### .eval() method\n",
    "The eval() lets you evaluate expressions on columns in a DataFrame. This can be particularly helpful for performing multiple operations at once or when building dynamic expressions.\n",
    "\n",
    "Pro tip: You can also create multiple columns in one go using a single eval statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   A  B  C   D\n",
      "0  1  4  5   4\n",
      "1  2  5  7  10\n",
      "2  3  6  9  18 \n",
      "\n",
      "0    1.0\n",
      "1    1.0\n",
      "2    1.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'A': [1, 2, 3],\n",
    "    'B': [4, 5, 6]\n",
    "})\n",
    "\n",
    "# Evaluate an expression and create a new column\n",
    "# df.eval(\"C = A + B\", inplace=True)\n",
    "# df.eval(\"D = A * B\", inplace=True)\n",
    "df.eval(\"\"\"\n",
    "    C = A + B\n",
    "    D = A * B\n",
    "\"\"\", inplace=True)\n",
    "print(df, \"\\n\")\n",
    "\n",
    "# Evaluate a complex expression\n",
    "result = df.eval(\"(A + B) / C\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### .assign() method\n",
    "The assign() makes it convenient to add new columns (or transform existing ones) without breaking up your method chain. You can keep your transformations “flowing” in a single pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    X  Y   Z  XY_sum\n",
      "0  10  2  20      12\n",
      "1  20  4  40      24\n",
      "2  30  8  60      38\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'X': [10, 20, 30],\n",
    "    'Y': [2, 4, 8]\n",
    "})\n",
    "\n",
    "# def calculate_z(df):\n",
    "#     return df['X'] * 2\n",
    "# def calculate_xy_sum(df):\n",
    "#     return df['X'] + df['Y']\n",
    "\n",
    "# df_assigned = df.assign(\n",
    "#     Z=calculate_z,\n",
    "#     XY_sum=calculate_xy_sum\n",
    "# )\n",
    "\n",
    "\n",
    "## Create new columns directly\n",
    "df_assigned = df.assign(\n",
    "    Z=lambda df: df['X'] * 2,   # Using a lambda\n",
    "    XY_sum=lambda df: df['X'] + df['Y']\n",
    ")\n",
    "print(df_assigned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### .explode() method\n",
    "When you have a column of lists (or arrays), explode() will unnest those lists into multiple rows, duplicating the other row values as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Id  Colors\n",
      "0   1     Red\n",
      "0   1    Blue\n",
      "1   2   Green\n",
      "1   2  Yellow\n",
      "2   3    Pink\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'Id': [1, 2, 3],\n",
    "    'Colors': [\n",
    "        ['Red', 'Blue'],\n",
    "        ['Green', 'Yellow'],\n",
    "        ['Pink']\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Explode the list in \"Colors\" into separate rows\n",
    "exploded_df = df.explode('Colors')\n",
    "print(exploded_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### .cut() method\n",
    "The cut() is typically used when you have numeric data and you want to split (or “bin”) it into intervals that you define. Think of it as creating buckets for your data values.\n",
    "\n",
    "Pro tip: If you don’t specify labels, cut() will create them automatically (like (0, 3], (3, 6], etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expenses:\n",
      "0    1000\n",
      "1     250\n",
      "2     300\n",
      "3     600\n",
      "4     750\n",
      "5     800\n",
      "6    1200\n",
      "7    2000\n",
      "dtype: int64\n",
      "\n",
      "Binned Expenses (using cut):\n",
      "0    (800, 2000]\n",
      "1       (0, 300]\n",
      "2       (0, 300]\n",
      "3     (300, 800]\n",
      "4     (300, 800]\n",
      "5     (300, 800]\n",
      "6    (800, 2000]\n",
      "7    (800, 2000]\n",
      "dtype: category\n",
      "Categories (3, interval[int64, right]): [(0, 300] < (300, 800] < (800, 2000]]\n",
      "\n",
      "Count of items in each bin:\n",
      "(300, 800]     3\n",
      "(800, 2000]    3\n",
      "(0, 300]       2\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample data: monthly expenses in dollars\n",
    "expenses = pd.Series([1000, 250, 300, 600, 750, 800, 1200, 2000])\n",
    "\n",
    "# Define bins: 0-300 = Low, 300-800 = Medium, 800-2000 = High\n",
    "# 'bins' is a list of the cut points; 'labels' give names to each bin\n",
    "binned_expenses = pd.cut(expenses,\n",
    "                         bins=[0, 300, 800, 2000],\n",
    "                        #  labels=['Low', 'Medium', 'High']\n",
    "                         )\n",
    "\n",
    "print(\"Expenses:\")\n",
    "print(expenses)\n",
    "print(\"\\nBinned Expenses (using cut):\")\n",
    "print(binned_expenses)\n",
    "\n",
    "# You can also see how many items fall into each bin\n",
    "print(\"\\nCount of items in each bin:\")\n",
    "print(binned_expenses.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### .qcut() method\n",
    "The qcut() is similar to cut(), but instead of defining fixed bin edges yourself, qcut() divides the data into bins based on quantiles (i.e., each bin has roughly the same number of data points).\n",
    "\n",
    "Pro tip: Adjust the q parameter to create the number of quantile-based bins you want (e.g., q=4 for quartiles)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expenses:\n",
      "0    1000\n",
      "1     250\n",
      "2     300\n",
      "3     600\n",
      "4     750\n",
      "5     800\n",
      "6    1200\n",
      "7    2000\n",
      "dtype: int64\n",
      "\n",
      "Quantile-Binned Expenses (using qcut):\n",
      "0      High\n",
      "1       Low\n",
      "2       Low\n",
      "3       Low\n",
      "4    Medium\n",
      "5    Medium\n",
      "6      High\n",
      "7      High\n",
      "dtype: category\n",
      "Categories (3, object): ['Low' < 'Medium' < 'High']\n",
      "\n",
      "Count of items in each bin:\n",
      "Low       3\n",
      "High      3\n",
      "Medium    2\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Same monthly expenses data\n",
    "expenses = pd.Series([1000, 250, 300, 600, 750, 800, 1200, 2000])\n",
    "\n",
    "# We'll split the data into 3 quantile-based bins\n",
    "quantile_binned_expenses = pd.qcut(expenses, q=3, labels=['Low', 'Medium', 'High'])\n",
    "\n",
    "print(\"Expenses:\")\n",
    "print(expenses)\n",
    "print(\"\\nQuantile-Binned Expenses (using qcut):\")\n",
    "print(quantile_binned_expenses)\n",
    "\n",
    "# Again, see how many items in each quantile-based bin\n",
    "print(\"\\nCount of items in each bin:\")\n",
    "print(quantile_binned_expenses.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q9: pivot_table() method\n",
    "\n",
    "> Q9: If you had to teach a new user about the pivot_table() method, what would you tell them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pivot_table() method in pandas is used to create a spreadsheet-style pivot table, allowing you to summarize and reorganize data. It aggregates data using a function (e.g., mean, sum), with options for indexing by rows (index) and columns (columns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         count             sum              mean              \n",
      "Store     East North West East North West   East  North   West\n",
      "Employee                                                      \n",
      "Alice        0     2    0    0   350    0    0.0  175.0    0.0\n",
      "Charlie      1     1    2  400   250  800  400.0  250.0  400.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample data: employee sales in different stores\n",
    "data = {\n",
    "    'Employee': ['Alice', 'Alice', 'Charlie', 'Charlie', 'Charlie', 'Charlie'],\n",
    "    'Store': ['North', 'North', 'West', 'East', 'North', 'West'],\n",
    "    'Sales': [200, 150, 300, 400, 250, 500]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "# Create a pivot table showing total sales by Employee and Store\n",
    "pivot = pd.pivot_table(\n",
    "    df,\n",
    "    index='Employee',                   # Row grouping\n",
    "    columns='Store',                    # Column grouping\n",
    "    values='Sales',                     # What we’re aggregating\n",
    "    aggfunc=['count', 'sum', 'mean'],   # How we aggregate\n",
    "    fill_value=0                        # Fill missing values with 0\n",
    ")\n",
    "\n",
    "print(pivot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q10: Python for Data Analysis\n",
    "\n",
    "> Q10: When is Python good for data analysis? When is it not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### When Python is Not the Best Option\n",
    "Python has its limitations and may not be the best choice in certain situations:\n",
    "\n",
    "1. Real-Time or High-Frequency Data Analysis  \n",
    "\t•\tPython is not the fastest language for real-time data processing or when ultra-low latency is required.  \n",
    "\t•\tAlternate Tools: Use C++, Java, or languages like Go for faster execution.  \n",
    "\t•\tExample: High-frequency trading systems that need microsecond-level latency.  \n",
    "\n",
    "2. Data Analysis in Spreadsheet-Heavy Workflows  \n",
    "\t•\tWhile Python supports Excel manipulation (openpyxl, xlwings), some tasks are faster and more intuitive in spreadsheet software.  \n",
    "\t•\tAlternate Tools: Excel or Google Sheets.  \n",
    "\t•\tExample: A small business performing quick financial modeling or one-off data summaries.  \n",
    "\n",
    "3. Large-Scale Business Intelligence Dashboards  \n",
    "\t•\tWhile Python can create visualizations, it lacks the interactivity and scalability of dedicated BI tools.  \n",
    "\t•\tAlternate Tools: Tableau, Power BI, or QlikView.  \n",
    "\t•\tExample: A corporate dashboard summarizing financial KPIs for executives.  \n",
    "\n",
    "4. Complex Statistical Analysis  \n",
    "\t•\tWhile Python has libraries like statsmodels and scipy, R is often preferred for advanced statistical modeling.  \n",
    "\t•\tAlternate Tools: R, SAS, or Stata.  \n",
    "\t•\tExample: A public health organization conducting epidemiological studies with R for survival analysis.  \n",
    "\n",
    "5. Handling Extremely Large Datasets  \n",
    "\t•\tPython may struggle with in-memory operations on large datasets.  \n",
    "\t•\tAlternate Tools: Apache Spark, Hadoop, or SQL-based solutions.  \n",
    "\t•\tExample: A social media company analyzing terabytes of user activity logs for pattern recognition.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### When Python is Good for Data Analysis\n",
    "Python is well-suited for data analysis in various scenarios due to its flexibility, extensive libraries, and community support:  \n",
    "\n",
    "1. Exploratory Data Analysis (EDA)  \n",
    "\t•\tPython excels at handling datasets for initial exploration.  \n",
    "\t•\tLibraries: pandas, matplotlib, seaborn, and plotly are commonly used for cleaning, exploring, and visualizing data.  \n",
    "\t•\tExample: A marketing team analyzing customer behavior data to identify trends using Jupyter Notebooks for visualization and insights.  \n",
    "\n",
    "2. Machine Learning Workflows  \n",
    "\t•\tPython is ideal for developing and testing machine learning models.  \n",
    "\t•\tLibraries: scikit-learn, TensorFlow, PyTorch, and XGBoost.  \n",
    "\t•\tExample: A retail company predicting customer churn using logistic regression or decision trees implemented in Python.  \n",
    "\n",
    "3. Big Data Processing  \n",
    "\t•\tPython, combined with libraries like PySpark or Dask, can manage large datasets for analysis.  \n",
    "\t•\tExample: An e-commerce platform analyzing clickstream data to optimize website layout for user engagement.  \n",
    "\n",
    "4. Automating Repetitive Data Tasks  \n",
    "\t•\tPython scripts can automate tasks like data extraction, transformation, and loading (ETL).  \n",
    "\t•\tLibraries: Airflow for workflows, requests for API interaction.  \n",
    "\t•\tExample: Automating the daily data extraction from APIs for dashboard updates in a logistics company.  \n",
    "\n",
    "5. Integration with Other Tools  \n",
    "\t•\tPython integrates well with databases (e.g., SQLAlchemy for SQL databases), cloud platforms, and visualization tools (e.g., Tableau or Power BI).  \n",
    "\t•\tExample: A fintech company querying databases with Python and sending processed data to Tableau dashboards.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q11: Performance Considerations\n",
    "\n",
    "> Q11: What are some performance considerations using Pandas? How do you optimize your code for speed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*\t**Avoid Row-by-Row Operations:** Row-by-row processing (e.g., loops or apply()) is slow and inefficient for large datasets.  \n",
    "\n",
    "*\t**Leverage Vectorized Operations:** Use built-in functions that operate on entire columns at once, leveraging Pandas’ C-optimized backend.  \n",
    "\n",
    "*\t**Optimize Data Types:** Reduce memory usage by using efficient data types (e.g., int8, float32) instead of defaults like int64 or float64.  \n",
    "\n",
    "*\t**Minimize Chained Operations:** Avoid chaining multiple operations without assigning intermediate results to variables.  \n",
    "\n",
    "*\t**Use Indexing Efficiently:** Use .loc[] or .iloc[] instead of chained indexing to avoid redundant DataFrame operations.  \n",
    "\n",
    "*\t**Consider Alternative Libraries for Large Datasets:** \n",
    "\t-\tUse Dask or modin for parallelized operations.  \n",
    "\t-\tUse NumPy for simpler data structures if advanced features are unnecessary.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q12: Handling Messy Data\n",
    "\n",
    "> Q12: Describe a situation where you've had to clean messy data. What were the challenges and how did you handle them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Situation:\n",
    "\n",
    "I worked on a project where I had to analyze customer feedback data from multiple sources, including survey results, emails, and social media comments. The dataset was messy due to inconsistent formats, missing values, duplicate entries, and a mix of structured and unstructured data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenges:\n",
    "\n",
    "1.\tInconsistent Formats:  \n",
    "•\tDate fields were in multiple formats (e.g., MM/DD/YYYY, DD-MM-YYYY).  \n",
    "•\tText fields had inconsistent capitalization and included emojis or special characters.  \n",
    "2.\tMissing Values:  \n",
    "•\tSome fields, such as customer age and location, were partially missing.  \n",
    "3.\tDuplicates:  \n",
    "•\tMultiple entries for the same customer appeared due to repeated survey submissions.  \n",
    "4.\tUnstructured Data:  \n",
    "•\tSocial media comments and emails contained free text that needed to be processed and analyzed.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Actions:\n",
    "1. Standardizing Data Formats:\n",
    "\t•\tUsed Pandas to standardize date formats and clean text data.\n",
    "2. Handling Missing Values:\n",
    "\t•\tFor numeric fields like age, used mean or median imputation.\n",
    "    •\tFor categorical fields like location, used the most frequent value or created a placeholder (\"Unknown\").\n",
    "3. Removing Duplicates:\n",
    "\t•\tIdentified and dropped duplicate rows based on a unique customer identifier.\n",
    "4. Processing Unstructured Data:\n",
    "\t•\tUsed Natural Language Processing (NLP) techniques to clean and extract insights from text data:\n",
    "\t•\tRemoved stop words.\n",
    "\t•\tTokenized text for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results:\n",
    "1.\tImproved Data Quality: Cleaned and standardized dataset was ready for analysis.\n",
    "2.\tActionable Insights: Generated customer sentiment analysis and identified key trends.\n",
    "3.\tScalable Workflow: Created a reusable data-cleaning pipeline for future projects.\n",
    "\n",
    "\n",
    "**Key Takeaways:**  \n",
    "\t•\tCleaning messy data requires identifying issues, applying systematic fixes, and ensuring consistency.  \n",
    "\t•\tAutomating processes with libraries like Pandas and NLP tools helps streamline data cleaning.  \n",
    "\t•\tA well-cleaned dataset leads to more reliable analysis and actionable insights.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "example",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
